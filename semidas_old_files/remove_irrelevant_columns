import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import griddata

# file location for merged yield dataset
q49_file = 'C:/Users/Inholland/Desktop/preject/data_set.csv'

# read file as dataframes
df3 = pd.read_csv(q49_file)


# rename columns in dataframe
df3 = df3.rename(columns={'Hoogte_BETA_20181202_version2': 'alt(m)_', 'qual_BETA_20181202_version2': 'qual_',
                          'speedkmh_BETA_20181202_version2': 'speed(km/h)_', 'loadkg_BETA_20181202_version2':
                          'load(kg)_', 'tarekg_BETA_20181202_version2': 'tare(kg)_', 'conv.facto_BETA_20181202_version2':
                          'conv.factor_', 'beltspdm_BETA_20181202_version2': 'beltspd(m/s)_',
                          'workwidth_BETA_20181202_version2': 'workwidth(m)_', 'yieldton_BETA_20181202_version2':
                          'yield(ton/ha)_', 'totalyield_BETA_20181202_version2': 'totalyield(ton)_',
                          'point_weig_BETA_20181202_version2': 'point weight (kg)_', 'totalarea_BETA_20181202_version2':
                          'totalarea(ha)_', 'worktimes_BETA_20181202_version2': 'worktime(s)_',
                          'loadnr_BETA_20181202_version2': 'loadnr_', 'loadweight_BETA_20181202_version2':
                          'loadweight(ton)_', 'tarecorrectedtotalyield(ton)': 'tarecorrectedtotalyield(_ton)',
                          'loadbeltm_BETA_20181202_version2': 'loadbelt(m)_', 'tarecorrec_BETA_20181202_version2':
                          'tarecorrectedyield(tonha)_', 'tarecorre2_BETA_20181202_version2':
                          'tarecorrectedtotalyield(_ton)_', 'tarecorrectedyield(ton/ha)': 'tarecorrectedyield(tonha)',
                          'speedkmh_BETA_20181202_version2': 'speed(km/h)','speed(km/h_C:\\Users\\Inholland\\Desktop\\preject\\CHARLIE_20181202': 'speed(km/h)',
                          'beltspdm_BETA_20181202_version2': 'beltspd(m/s)_', 'beltspd(m/_C:\\Users\Inholland\Desktop\preject\CHARLIE_': 'beltspd(m/s)_',
                          'workwidth_BETA_20181202_version2': ' workwidth', 'workwidth_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': ' workwidth',
                          'yieldton_BETA_20181202_version2': 'yield(ton/ha)','yield(ton/_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'yield(ton/ha)',
                          'totalyield_BETA_20181202_version2': 'totalyield', 'totalyield_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'totalyield',
                          'point_weig_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'point_weig', 'point_weig_BETA_20181202_version2': 'point_weig',
                          'totalarea_BETA_20181202_version2': 'totalarea', 'totalarea(_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'totalarea',
                          'worktime(s_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': ' worktime ', 'worktimes_BETA_20181202_version2': 'worktime ',
                          'loadweight_BETA_20181202_version2':'loadweight','loadbeltm_BETA_20181202_version2':'loadbeltm',
                          'loadbelt(m_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202':'loadbeltm','EC_0.5m_C:\\Users\Inholland\Desktop\preject\ALPHA_20181202':'EC_0.5m',
                          'EC_0.5m_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202':'EC_0.5m','EC_1m_C:\\Users\Inholland\Desktop\preject\ALPHA_20181202': 'EC_1m',
                          'EC_1.5m_C:\\Users\Inholland\Desktop\preject\ALPHA_20181202':'EC_1.5m','EC_3m_C:\\Users\Inholland\Desktop\preject\ALPHA_20181202':'EC_3m',
                          'EC_3m_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'EC_3m','EC_1m_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'EC_1m',
                          'EC_1.5m_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'EC_1.5m',
                          'loadweight_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202':'loadweight',
                          'Hoogte_C:\\Users\Inholland\Desktop\preject\ALPHA_20181202': 'Hoogte', 'Hoogte2_C:\\Users\Inholland\Desktop\preject\CHARLIE_20181202': 'Hoogte'})

# list of regex word for merging columns
column_list = ['alt(m)', 'qual', 'speed(km/h)', 'load(kg)', 'tare(kg)', 'conv.factor', 'beltspd(m/s)', 'workwidth(m)',
               'yield(ton/ha)', 'totalyield(ton)', 'point weight', 'totalarea(ha)', 'worktime(s)', 'loadnr', 'loadweight(ton)',
               'loadbelt(m)', 'tarecorrectedyield(tonha)', 'tarecorrectedtotalyield(_ton)','beltspd(m/s)_', 'workwidth','yield(ton/ha)','totalyield','point_weig',
               'totalarea','worktime ', 'loadbeltm','EC_0.5m','EC_1m', 'EC_3m', 'EC_1.5m','loadweight', 'Hoogte']


# merging columns by taking the mean of each corresponding columns values
def merge_columns(dataframe, column_regex_list):
    for column_name in column_regex_list:
        column_list = [col for col in dataframe.columns if column_name in col]
        dataframe['mean_' + '%s' % column_name] = dataframe[column_list].mean(axis=1)
        dataframe = dataframe.drop(column_list, axis=1)
        print(column_list)
    return dataframe

# visualize the correlation using heatmap
def heatmap(num_of_columns, dataframe):
    k = num_of_columns #number of variables for heatmap
    corrmat = dataframe.corr()
    cols = corrmat.nlargest(k, 'mean_yield(ton/ha)')['mean_yield(ton/ha)'].index
    cm = np.corrcoef(dataframe[cols].values.T)
    sns.set(font_scale=1)
    hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)
    plt.show()
    return

# amount of columns with duplicates
len(df3.columns)

# merging the columns
merged_df = merge_columns(df3, column_list)

# amount of columns after merging
len(merged_df.columns)

# drop useless/redundent columns
df = merged_df.drop(['Unnamed: 0', 'year', 'month', 'day', 'hr', 'min', 'sec', 'mean_alt(m)', 'mean_tare(kg)',
                     'mean_conv.factor', 'usertare(%)'], axis=1)

# amount of columns after drop
len(df.columns)

# visualize the correlations
heatmap(len(merged_df.columns), merged_df)

# drop identical columns
df = df.drop(['mean_worktime(s)', 'mean_tarecorrectedtotalyield(_ton)', 'x(m)', 'y(m)'], axis=1)

# amount of columns in dataftame
len(df.columns)

# visualize correlation
heatmap(16, merged_df)

# save to CSV
merged_df.to_csv('C:/Users/Inholland/Desktop/final_merged_data.csv')

len(merged_df.columns)
